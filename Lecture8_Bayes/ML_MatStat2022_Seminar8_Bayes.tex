\documentclass[9pt]{beamer}




\usepackage{beamerthemesplit}
\usetheme{Boadilla}
\usecolortheme{orchid}





 % \usepackage[cp866]{inputenc}                %%% Є®¤Ёа®ўЄ  DOS
  \usepackage[cp1251]{inputenc}
%  \usepackage[T2A]{fontenc}                %%% ???
%\usepackage[english,russian]{babel}
%\usepackage[russian]{babel}
 \usepackage[OT1]{fontenc}
%\usepackage[english]{babel}
\usepackage[russian]{babel}

\usepackage{amssymb,latexsym, amsmath, mathdots}

\usepackage{amscd}
\usepackage{multirow}
\usepackage{comment}

\usepackage{epstopdf}



%\usepackage[table]{xcolor} %colored cells

%\usepackage{tikz}
%\usetikzlibrary{tikzmark} %arrows in tables


\usepackage{mnsymbol} % udots in matrices


 \usepackage{pgfpages}
%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=bottom}
%% \setbeameroption{show notes on second screen}

%\setbeamercolor{alerted text}{fg=red!80!black}


% \setbeamercolor{alerted text}{fg=blue!50!green}
% \setbeamercolor{fortheorem}{bg=blue!15!white}
% \setbeamercolor{EMPF}{bg=blue!25!white}
% \setbeamercolor{EMPF2}{bg=blue!35!white}

%\setbeamersize{text margin left=5.mm, text margin right=5.mm}


%---------------------------------------------------------------------------------------------------------------------------

%\def\emphbox#1#2#3{\begin{beamercolorbox}[wd=#2, ht=#1, center, colsep=1.mm]{EMPF} #3 \end{beamercolorbox}}

%---------------------------------------------------------------------------------------------------------------------------


%\advance\leftskip-5.mm



%***************************************************************************************************************************

\theoremstyle{theorem}
\newtheorem{mytheorem}[theorem]{Теорема}
\newtheorem{mylemma}[theorem]{Лемма}
\newtheorem{mycorollary}[theorem]{Следствие}

\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{assertion}[theorem]{Утверждение}
\newtheorem{remark}[theorem]{Замечание}
\newtheorem{assumption}[theorem]{Предположение}

\newtheorem{convention}[theorem]{Договорённость}
\newtheorem{question}[theorem]{Вопрос}
%\newtheorem{mydefinition}{mydefinition}

\theoremstyle{definition}
\newtheorem{mydefinition}[theorem]{Определение}
\newtheorem{myexample}[theorem]{Пример}



%***************************************************************************************************************************

\chardef\No=194
\newcommand{\bd}{{\rm bd}}
\newcommand{\cl}{{\rm cl}}
\newcommand{\ind}{{\rm ind}}
\newcommand{\id}{{\rm id}}
\newcommand{\inj}{{\sl in}}
\newcommand{\pr}{{\rm pr}}
\newcommand{\st}{{\rm St}}


\DeclareMathOperator{\sgrad}{sgrad}

\DeclareMathOperator{\cnt}{const}

\DeclareMathOperator{\Aut}{Aut}


\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\Int}{Int}


\DeclareMathOperator{\tr}{tr}

\DeclareMathOperator{\Mat}{Mat}







%****************************************************************************
\newcommand{\Ker}{\mathrm{Ker}\,\,}
\newcommand{\Ann}{\mathrm{Ann}\,\,}

\newcommand{\Imm}{\mathrm{Im}\,\,}

\newcommand{\rk}{\mathrm{rk}\,\,}

\newcommand{\const}{\mathrm{const}}



\DeclareMathOperator{\Pen}{\mathcal{P}}


\DeclareMathOperator{\Core}{K}


\DeclareMathOperator{\BLG}{\operatorname{BLG}}

\DeclareMathOperator{\AutBP}{\operatorname{Aut}(V, \mathcal{P})}


\def\minus{\hbox{-}}   %Sign of minus in big matrices

\usepackage{multirow}  %Columns spanning multiple rows

\usepackage[normalem]{ulem} %underline that can break lines

%\usepackage[normalem]{ulem}

%****************************************************************************



\title% [] (optional, only for long titles)
{Прикладная статистика в машинном обучении \\ Семинар 8 \\ Байесовские методы}
%\subtitle{}
\author [И. \,К.~Козлов] %(optional, for multiple authors)
{И. \,К.~Козлов \\ {\footnotesize(\textit{Мехмат МГУ})}}


\date % [](optional)
{2022}
%\subject{Mathematics}







\usepackage{hyperref}
%\hypersetup{unicode=true}
\hypersetup{
  colorlinks=true,
  linkcolor=green!70!black, %blue!50!red,
  urlcolor=green!70!black,
  unicode=true
}



\begin{document}

%\includeonlyframes{}
\frame{\titlepage}



\section{Ошибка прокурора}

\begin{frame}[plain]\frametitle{Ошибка прокурора} \tableofcontents[currentsection]\end{frame}




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Prosecutor's fallacy}
%----------------------------------------------------------

\begin{center}
\Huge Ошибка прокурора 
\end{center}

\begin{figure}[h!]
\center{\includegraphics[height=0.5\textheight]{pic/Edge1}}
%\caption{\Large Осторожно! Грядут страшные формулы!} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Ошибка прокурора}
%----------------------------------------------------------

\begin{itemize}

\item ДНК подсудимого совпал с ДНК убийцы.

\vspace{2mm}

\item Вероятность этого 1 на 2 млн.

\vspace{2mm}

\item Эрго: подсудимый виновен. 

\vspace{2mm}

\textit{Вероятность подобного события при его невиновности астрономически мала.}

\end{itemize}

\begin{figure}[h!]
\center{\includegraphics[height=0.5\textheight]{pic/Guilt1}}
%\caption{\Large Осторожно! Грядут страшные формулы!} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Ошибка прокурора}
%----------------------------------------------------------

\begin{center}
\Huge \textbf{Q:} Где обман?
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.75\textwidth]{pic/Objection}}
%\caption{\LARGE Objection!} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Ошибка прокурора}
%----------------------------------------------------------

Пусть $\operatorname{E} = \operatorname{Evidence}, \, \operatorname{I} = \operatorname{Innocence}$.

\vspace{5mm}


{\color{blue} Ошибка прокурора}:

\[ \mathbb{P}(\operatorname{I} \mid \operatorname{E}  ) \not  = \mathbb{P}(\operatorname{E} \mid \operatorname{I} ) \]


\vspace{5mm}

Прокурор не выучил {\color{blue} формулу Байеса}:

\[  \mathbb{P}(\operatorname{I} \mid \operatorname{E}  ) = \mathbb{P}(\operatorname{E} \mid \operatorname{I} ) \, \, \frac{\mathbb{P}( \operatorname{I} )}{ \mathbb{P}(\operatorname{E} )}\]



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Ошибка прокурора}
%----------------------------------------------------------

\textbf{Наглядный пример}. Пусть вероятность выиграть в лотерею: 1 на 2 млн.

\vspace{5mm}

Отсюда $\not \Rightarrow$ победитель --- жулик.

\vspace{5mm}

С ДНК тестом в 10 млн городе в среднем 5 человек с тем же ДНК.

\vspace{5mm}

Среди них вероятность вины не $\displaystyle \frac{1}{2\text{млн}}$, а $20\%$.


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Ошибка прокурора}
%----------------------------------------------------------

{\color{red} Ошибка прокурора} --- реальная ошибка:

\vspace{5mm}

\begin{itemize}

\item Салли Кларк --- осуждена (1999) из-за смерти двух сыновей в младенчестве.

\vspace{5mm}

\item Люсия де Берк --- медсестра, была осуждена (2003) из-за 13 подозрительных смертей во время её дежурства.


\end{itemize}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------






\section{Парадокс Монти-Холла}

\begin{frame}[plain]\frametitle{Парадокс Монти-Холла} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------


\begin{center}
\textbf{Парадокс Монти Холла}
\end{center}



\begin{itemize}

\item Игра: нужно выбрать 1 из 3 дверей. За одной дверью - автомобиль, за 2 другими - козы.

\vspace{2mm}


\begin{figure}[h!]
\center{\includegraphics[width=0.6\textwidth]{pic/montyhallproblem}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

\pause

\item Выбрали дверь. Затем {\color{blue} ведущий открывает одну из оставшихся дверей, за которой находится коза}.


\vspace{2mm}


\item {\color{red} Ведущий знает, где автомоболь}.
\vspace{2mm}

\item \textbf{Q:} Увеличится ли вероятность выиграть, если изменить теперь свой выбор?

\end{itemize}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------

Главное --- \textbf{правильно посчитать вероятности и зависимость между событиями}:

\vspace{5mm}


\begin{itemize}

\item автомобиль равновероятно размещён за любой из трёх дверей;


\vspace{5mm}


\item {\color{blue} ведущий всегда открывает дверь, за которой коза;}


\vspace{5mm}

\item {\color{red} ведущий знает, где находится автомобиль;}

\vspace{5mm}

\item если игрок сразу выбрал правильную дверь, ведущий открывает любую из оставшихся с одинаковой вероятностью.

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------

\begin{center}
\textbf{A:} {\color{blue} Лучше изменить выбор. }

\vspace{2mm}


Наглядно --- пусть дверей 1000. Открывают 998  из оставшихся 999. 

\vspace{2mm}

Выбор очевиден? 
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.6\textwidth]{pic/Monty2}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------



\begin{center}
\Large Решение ``в лоб'' --- посчитать все вероятности:
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{pic/Monty3}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}






%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------

\begin{center}
\Large Применим {\color{blue} теорему Байеса}!
\end{center}

\vspace{5mm}

\Large А какие в задаче  априорные и апостериорные вероятности?

\vspace{5mm}

\begin{itemize}

\item \Large  \textbf{Априорные вероятности} \[ 1: 1:1\]

\vspace{2mm}

\item \Large  Пусть выбираем 1ую дверь. Вероятности не изменились.

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------

\begin{itemize}

\item \large  Считаем, что ведущий открыл 3ью дверь. 

\vspace{5mm}

\item \textbf{Q:} Правдоподобие? (Вероятности этого события)

\vspace{5mm}

\pause 

\item Обозначим события:

\vspace{5mm}
\begin{itemize}

\item $Ai =$ Автомобиль за дверью i 

\vspace{5mm}

\item $O =$ открыли дверь 3.

\end{itemize}

\vspace{5mm}

\item \textbf{Правдоподобие}: \[ \mathbb{P}(O \mid Ai ) = (50\%, \quad 100\%, \quad 0\%)\]
 

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------


\begin{itemize}

\item \large Чему пропорциональна апостериорная вероятность по теореме Байеса:

\[  \mathbb{P}(\operatorname{Ai} \mid O  ) \propto \mathbb{P}(\operatorname{O}  \mid \operatorname{Ai}) \, \, \mathbb{P}( \operatorname{Ai} ).\]


\vspace{5mm}

\pause

\item \textbf{Апостериорная вероятность}: \[ 1:2:0\]
 
 В 2 раза больше вероятность выиграть с другой дверью.

\end{itemize}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Парадокс Монти Холла}
%----------------------------------------------------------


\begin{center}
\Large  Если Вы выбрали {\color{red} другую дверь}, то ...
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{pic/Car}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




\section{Графические модели}

\begin{frame}[plain]\frametitle{Графические модели} \tableofcontents[currentsection]\end{frame}



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Графические модели}
%----------------------------------------------------------

Зависимость между переменными удобно изображать {\color{blue} ориентированным ациклическим графом} ( directed acyclic graph = DAG) $G  = \left(V, D\right)$. 

\vspace{2mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/DAG1}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

\vspace{2mm}

\pause

\begin{itemize}

\item Вершины $v \in V$ соответствуют случайным величинам. 

\vspace{2mm}

\item Рёбра $e \in D$ обозначают статистическую зависимость. 


\vspace{2mm}

В графе {\color{red} не должно быть циклов}.

\vspace{2mm}

\item Для каждой вершины задаётся условная вероятность \[ p(x_i \mid x_{\operatorname{parents}(i)}).\]

\end{itemize}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Оказывается, мы говорили прозой...}
%----------------------------------------------------------

\begin{center}
\Large DAG --- настолько естественное понятие,  что \\ мы уже использовали его на прошлом занятии:
\end{center}

\vspace{5mm}


\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{pic/Late}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Оказывается, мы говорили прозой...}
%----------------------------------------------------------


Полная вероятность \textbf{факторизуется} --- распадается в произведения локальных распределений:
\[ p(x_1,\ldots,x_n) = \prod_{i\in V} p(x_i|x_{\text{parents}(i)})\]

\vspace{3mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{pic/DAG2}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

\vspace{3mm}

Например, для графа на рис. \[ p(x_A, x_B, x_C, x_D) = p(x_A)p(x_B|x_A)p(x_C|x_B)p(x_D|x_C).\]


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Графические модели}
%----------------------------------------------------------


\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{pic/Gr1}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Маргинализация и обуславливание}
%----------------------------------------------------------

\textit{Общее правило}. Пусть мы хотим вычислить $p(x)$ по $p(x, y)$.

\vspace{5mm}

\begin{itemize}

\item Если $y$ \textbf{неизвестно}, то мы {\color{blue} маргинализуем} по ним: \[ p(x) = \int p(x, y) dy\]

\vspace{5mm}


\item Если $y$ \textbf{известно}, то мы {\color{blue} обуславливаем} плотность по нему: \[ p(x \mid y) = \frac{p(x, y) }{p(y)}.\]



\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Маргинализация и обуславливание}
%----------------------------------------------------------

\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{pic/Sol1}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


\section{Медицинский тест}

\begin{frame}[plain]\frametitle{Медицинский тест} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Медицинский тест}
%----------------------------------------------------------

\begin{itemize}

\item Болезнью болеет $10\%$ населения \[ \mathbb{P}(D) = 10\%, \qquad \mathbb{P}(\neg D) = 90\%\]

\vspace{5mm}

\item Тест правильно определяет болен/не болен в $90\%$ случаев: \[ \mathbb{P}(+ \mid D) = 90\%, \qquad \mathbb{P}(- \mid \neg D) = 90\%\] 

\vspace{5mm}

\pause

\item Тест положительный! \textbf{Q:} Какова вероятность болезни?
\end{itemize}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Медицинский тест}
%----------------------------------------------------------

\textbf{A:} Теорема Байеса показывает, что не такая большая:


\[ \mathbb{P} [D \lvert +]= \frac{ \mathbb{P}( + \mid D) \mathbb{P}(D)}{\mathbb{P}( + \mid D) \mathbb{P}(D) + \mathbb{P}( + \mid \neg D) \mathbb{P}( \neg D)} = \] \[ = \frac{0.09}{0.09 +0.09}=50 \%\]

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Медицинский тест}
%----------------------------------------------------------



\begin{center}
\Large Как ``графически'' выглядит теорема Байеса:
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/Bayes4}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Теорема Байеса графически}
%----------------------------------------------------------


\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/Bayes5}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Перерыв}
%----------------------------------------------------------


\begin{center}
\Huge Перерыв
\end{center}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


\section{QDA и LDA}

\begin{frame}[plain]\frametitle{QDA и LDA} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ML пример}
%----------------------------------------------------------

Сегодня мы уже применяли \textbf{теорему Байеса} для классификации: \[  \mathbb{P}\left( \operatorname{Cat} \mid \operatorname{Data} \right) =  \frac{ {\color{blue} \mathbb{P}\left(\operatorname{Data} \mid \operatorname{Cat} \right) \mathbb{P}\left( \operatorname{Cat}\right)} }{{\color{blue}  \mathbb{P}\left(\operatorname{Data} \mid \operatorname{Cat} \right) \mathbb{P}\left( \operatorname{Cat}\right) } + \mathbb{P}\left(\operatorname{Data} \mid \operatorname{Dog} \right) \mathbb{P}\left( \operatorname{Dog}\right)} \] 

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{QDA}
%----------------------------------------------------------

Аналогично в общем виде: 

\[P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}\]

\vspace{2mm}


\textbf{Q:} Какое самое правдоподобие $P(x \mid y)$ взять?


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{QDA}
%----------------------------------------------------------

\begin{itemize}

\item  {\color{blue} Quadratic Discriminant Analysis} (QDA): 

\[P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)\right)\]


\pause

\vspace{5mm}

\item QDA очень прост в вычислении: \[\begin{split}\log P(y=k | x) &= \log P(x | y=k) + \log P(y = k) + \operatorname{const} = \\
&= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + \operatorname{const},\end{split}\]

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{LDA}
%----------------------------------------------------------

\begin{itemize}

\item  {\color{blue}Linear Discriminant Analysis} (LDA) --- предполагаем, что все матрицы ковариации совпадают $\Sigma_k = \Sigma$. 


\vspace{5mm}

Формулы становятся линейными:  

\[\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) + \log P(y = k) + \operatorname{const}.\]


\vspace{5mm}

\pause

Логарифм апостериорного линеен по $x$: \[\log P(y=k | x) = \omega_k^T  x + \omega_{k0} + \operatorname{const}.\] \[ \omega_k = \Sigma^{-1} \mu_k, \qquad \omega_{k0} =
-\frac{1}{2} \mu_k^T \Sigma^{-1}\mu_k + \log P (y = k).\]

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{QDA}
%----------------------------------------------------------

Реализация QDA и LDA: \url{https://scikit-learn.org/stable/modules/lda_qda.html} 


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------
\begin{frame}[plain]\frametitle{QDA}
%----------------------------------------------------------

Логистическая регрессия и LDA оба приводят к линейному классификационному правилу \[ \log\left( \frac{\mathbb{P} (Y=1 \mid X=x)}{\mathbb{P} (Y=0 \mid X=x)}\right) = \beta_0 + \beta_1 x\]

\vspace{5mm}

\textbf{Разница}: LogReg --- дискриминативная модель, а LDA --- генеративная. 


\begin{figure}[h!]
\center{\includegraphics[width=0.45\textwidth]{pic/LogLDA}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------

\section{Наивный Байес}

\begin{frame}[plain]\frametitle{Наивный Байес} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Наивный Байес}
%----------------------------------------------------------

\begin{center} 
\Large {\color{blue} Naive Bayes classifier}
\end{center}

\vspace{5mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.4\textwidth]{pic/NaiveBayes}}
%\caption{} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Наивный Байес}
%----------------------------------------------------------

\begin{center}
\Large Наивный пример.
\end{center}


\begin{itemize}

\item У нас есть 3 класса --- пёсик, мячик и воздушный шарик.

\vspace{2mm}

\item Мы знаем, что объект --- синий. \textbf{Q:} Какой класс наиболее вероятен?

\end{itemize}


\begin{figure}[h!]
\center{\includegraphics[width=0.7\textwidth]{pic/NB1}}
%\caption{} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Наивный Байес}
%----------------------------------------------------------



\begin{itemize}

\item Итак, знаем фичу $\Rightarrow$ классы более или менее вероятны.

\vspace{2mm}

\item \textbf{Q:} А что делать, если фичей несколько?

\vspace{2mm}

\pause

\item \textbf{A:} Самое простое --- перемножить вероятности, как будто фичи независимы.

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ Naive Bayes}
%----------------------------------------------------------


\begin{itemize}

\item Даны $n$ фичей --- вектор ${\displaystyle \mathbf {x} =(x_{1},\ldots ,x_{n})}$. Мы хотим вычислить вероятность класса:

\[P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots, x_n \mid y)}
                                 {P(x_1, \dots, x_n)}\]
                                 
       
       \vspace{5mm}                          
                                 
                                 
                                 
   \item    Предположение {\color{blue} Наивного Байеса} --- \textit{фичи условно независимы} \[ P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y).\]
   
   
 
            \end{itemize}                     
           
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ Naive Bayes}
%----------------------------------------------------------  
   
        
Получаем классификационное правило:  \begin{align}\begin{aligned}P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)\\\Downarrow\\\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),\end{aligned}\end{align}
           
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ Naive Bayes}
%----------------------------------------------------------                      
                                 
                                 
Реализация Naive Bayes: \url{https://scikit-learn.org/stable/modules/naive_bayes.html} 

          
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ Naive Bayes}
%----------------------------------------------------------                      
                                
      
\begin{itemize}

\item Пусть $X_1, \dots, X_n$ --- независимые нормальные распределения $ {\displaystyle X_{k}\sim \ {\mathcal {N}}(\mu_i,\sigma_i^2)}$. 

\vspace{5mm}

\item \textbf{Q:} Какое распределение у вектора $(X_1, \dots, X_n)$?

\vspace{5mm}

\item \textbf{A:}  Многомерное нормальное распределение ${\mathcal {N}}(\mu, \Sigma)$, где \[ \mu = \left( \mu_1,\dots, \mu_n\right), \qquad  \Sigma = \left(\begin{matrix}\sigma_{1}^2\\&\ddots \\&&\sigma_{n}^2\end{matrix}\right).\]

\end{itemize}    


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ Naive Bayes}
%----------------------------------------------------------                      
         
         \begin{center}
         \Large {\color{blue} Gaussian Naive Bayes}
         \end{center}

\vspace{2mm}         
                                
\begin{itemize}

\item Непрерывные фичи можно приближать гауссовским распределением. 

     \vspace{5mm}

\item \textbf{Q:} Какое распределение у классов?  

     \vspace{5mm}
     
\pause     

\item \textbf{A:} Нормальное с одной и той же {\color{red} диагональной} матрицей $\Sigma$. 

\end{itemize}        
                                                        
             \vspace{5mm}
             
                            
\begin{center}
\noindent\fbox{%
    \parbox{23em}{%


\vspace{3mm}



    \Huge \textbf{\hspace{1mm} Gaussian Naive Bayes = \\ = LDA with diagonal $\Sigma$}
    
\vspace{3mm}    
    }%
}


\end{center}                
                
                                
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Наивный Байес --- неплохой бейзлайн}
%----------------------------------------------------------    

\begin{center}
\large При всей своей ``наивности'', Naive Bayes обладает \\  рядом {\color{green} существенных преимуществ}:
\end{center}


\vspace{2mm}

\begin{itemize}

\item Очень простой. Обучается даже на малых выборках. \textbf{Неплохой бейзлайн}.

\vspace{3mm}

\item {\color{blue} Очень быстрый}. Даже на очень больших данных.

\end{itemize}

\vspace{3mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{pic/Sonic}}
%\caption{} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Когда скорость решает}
%----------------------------------------------------------    

\begin{center}

\Large \textbf{Q:} Что в IT нужно классифицировать очень быстро, ``влёт''?

\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{pic/NBA}}
%\caption{} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------






\end{document}


