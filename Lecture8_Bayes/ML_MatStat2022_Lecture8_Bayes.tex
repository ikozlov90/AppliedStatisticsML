\documentclass[9pt]{beamer}




\usepackage{beamerthemesplit}
\usetheme{Boadilla}
\usecolortheme{orchid}





 % \usepackage[cp866]{inputenc}                %%% Є®¤Ёа®ўЄ  DOS
  \usepackage[cp1251]{inputenc}
%  \usepackage[T2A]{fontenc}                %%% ???
%\usepackage[english,russian]{babel}
%\usepackage[russian]{babel}
 \usepackage[OT1]{fontenc}
%\usepackage[english]{babel}
\usepackage[russian]{babel}

\usepackage{amssymb,latexsym, amsmath, mathdots}

\usepackage{amscd}
\usepackage{multirow}
\usepackage{comment}

\usepackage{epstopdf}



%\usepackage[table]{xcolor} %colored cells

%\usepackage{tikz}
%\usetikzlibrary{tikzmark} %arrows in tables


\usepackage{mnsymbol} % udots in matrices


 \usepackage{pgfpages}
%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=bottom}
%% \setbeameroption{show notes on second screen}

%\setbeamercolor{alerted text}{fg=red!80!black}


% \setbeamercolor{alerted text}{fg=blue!50!green}
% \setbeamercolor{fortheorem}{bg=blue!15!white}
% \setbeamercolor{EMPF}{bg=blue!25!white}
% \setbeamercolor{EMPF2}{bg=blue!35!white}

%\setbeamersize{text margin left=5.mm, text margin right=5.mm}


%---------------------------------------------------------------------------------------------------------------------------

%\def\emphbox#1#2#3{\begin{beamercolorbox}[wd=#2, ht=#1, center, colsep=1.mm]{EMPF} #3 \end{beamercolorbox}}

%---------------------------------------------------------------------------------------------------------------------------


%\advance\leftskip-5.mm



%***************************************************************************************************************************

\theoremstyle{theorem}
\newtheorem{mytheorem}[theorem]{Теорема}
\newtheorem{mylemma}[theorem]{Лемма}
\newtheorem{mycorollary}[theorem]{Следствие}

\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{assertion}[theorem]{Утверждение}
\newtheorem{remark}[theorem]{Замечание}
\newtheorem{assumption}[theorem]{Предположение}

\newtheorem{convention}[theorem]{Договорённость}
\newtheorem{question}[theorem]{Вопрос}
%\newtheorem{mydefinition}{mydefinition}

\theoremstyle{definition}
\newtheorem{mydefinition}[theorem]{Определение}
\newtheorem{myexample}[theorem]{Пример}



%***************************************************************************************************************************

\chardef\No=194
\newcommand{\bd}{{\rm bd}}
\newcommand{\cl}{{\rm cl}}
\newcommand{\ind}{{\rm ind}}
\newcommand{\id}{{\rm id}}
\newcommand{\inj}{{\sl in}}
\newcommand{\pr}{{\rm pr}}
\newcommand{\st}{{\rm St}}


\DeclareMathOperator{\sgrad}{sgrad}

\DeclareMathOperator{\cnt}{const}

\DeclareMathOperator{\Aut}{Aut}


\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\Int}{Int}


\DeclareMathOperator{\tr}{tr}

\DeclareMathOperator{\Mat}{Mat}







%****************************************************************************
\newcommand{\Ker}{\mathrm{Ker}\,\,}
\newcommand{\Ann}{\mathrm{Ann}\,\,}

\newcommand{\Imm}{\mathrm{Im}\,\,}

\newcommand{\rk}{\mathrm{rk}\,\,}

\newcommand{\const}{\mathrm{const}}



\DeclareMathOperator{\Pen}{\mathcal{P}}


\DeclareMathOperator{\Core}{K}


\DeclareMathOperator{\BLG}{\operatorname{BLG}}

\DeclareMathOperator{\AutBP}{\operatorname{Aut}(V, \mathcal{P})}


\def\minus{\hbox{-}}   %Sign of minus in big matrices

\usepackage{multirow}  %Columns spanning multiple rows

\usepackage[normalem]{ulem} %underline that can break lines

%\usepackage[normalem]{ulem}

\usepackage{wasysym} %smilies

%****************************************************************************



\title% [] (optional, only for long titles)
{Прикладная статистика в машинном обучении \\ Лекция 8 \\ Байесовские методы}
%\subtitle{}
\author [И. \,К.~Козлов] %(optional, for multiple authors)
{И. \,К.~Козлов \\ {\footnotesize(\textit{Мехмат МГУ})}}


\date % [](optional)
{2022}
%\subject{Mathematics}







\usepackage{hyperref}
%\hypersetup{unicode=true}
\hypersetup{
  colorlinks=true,
  linkcolor=green!70!black, %blue!50!red,
  urlcolor=green!70!black,
  unicode=true
}

\begin{document}

%\includeonlyframes{}
\frame{\titlepage}







\section{Теорема Байеса}

\begin{frame}[plain]\frametitle{Теорема Байеса} \tableofcontents[currentsection]\end{frame}

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Be afraid, be very afraid}
%----------------------------------------------------------

\begin{center} \Large Мы закончили с частотным подходом к статистике.

\vspace{5mm}

Перейдём к {\color{blue} байесовским методам} и {\color{blue} генеративным моделям}. \end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{pic/Kansas}}
%\caption{} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Условная вероятность}
%----------------------------------------------------------


\begin{center}
\LARGE Условная вероятность:
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{pic/Cond1}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Something wicked this way comes}
%----------------------------------------------------------

\begin{center}
\Large Выразим совместную вероятность событий двумя способами: \[ P(A\cap B) = P(A \mid B) P(B) = P(B \mid A) P(A)\] 

Вглядитесь --- сейчас мы узрим одну из важнейших формул курса!
\end{center}


\vspace{3mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.55\textwidth]{pic/Fry}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Теорема Байеса}
%----------------------------------------------------------


\begin{center}
\LARGE \textbf{Теорема Байеса}:
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{pic/Bayes1}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Формула полной вероятности}
%----------------------------------------------------------

Чтобы применить теорему Баейеса, вспомним важный факт.

\vspace{3mm}

\textbf{\color{red} Формула полной вероятности}. Пусть $A_1, \dots, A_k$ --- разбиение пространства элементарных событий $\Omega$, т.е. $A_i$ измеримы и \[ \Omega = \bigcup_{i=1}^k A_i, \qquad A_i \cap A_j = \emptyset, \quad i \not= j. \]  Тогда для любого события $B$ выполнено \[ {\color{blue} \mathbb{P}(B) = \sum_{i=1}^k \mathbb{P} (B \mid A_i) \mathbb{P}(A_i)}.\]

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Формула полной вероятности}
%----------------------------------------------------------

\begin{figure}[h!]
\center{\includegraphics[width=0.45\textwidth]{pic/Total}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

\vspace{3mm}


\textbf{Доказательство}. События $B\cap A_i$ попарно не пересекаются и вместе образуют $B$, поэтому \[  \mathbb{P}(B)  = \sum_{i=1}^k  \mathbb{P}(B\cap A_i) =  \sum_{i=1}^k \mathbb{P} (B \mid A_i) \mathbb{P}(A_i).\] 




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Формула Байеса}
%----------------------------------------------------------


Применим формулу полной вероятности к формуле Байеса: \[{\displaystyle \mathbb{P}(A_{i}\mid B)={\frac {\mathbb{P}(A_{i}) \mathbb{P}(B\mid A_{i})}{\sum _{j=1}^{k}\mathbb{P}(A_{j}) \mathbb{P}(B\mid A_{j})}}} \] 



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ML пример}
%----------------------------------------------------------

\large \textbf{Пример}. Применение теоремы Байеса в ML.

\vspace{3mm}

У нас есть данные $\operatorname{Data}$ и мы хотим отличить котика от пёсика. 

\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/CatDog}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

\vspace{3mm}

\textbf{Q:} Какую вероятность мы хотим посчитать?

\vspace{3mm}

\pause

\textbf{A:} Вероятность класса при наблюдаемых данных: \[\mathbb{P}\left( \operatorname{Cat} \mid \operatorname{Data}\right).\]





%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ML пример}
%----------------------------------------------------------

\begin{itemize}

\item В жизни проверить --- подходит ли описание котику \[ \mathbb{P}\left( \operatorname{Data} \mid \operatorname{Cat}\right)\] гораздо проще, чем угадать --- какое животное описано \[ \mathbb{P}\left( \operatorname{Cat} \mid \operatorname{Data} \right).\] 


\item Также несложно оценить доли котиков и пёсиков: \[\mathbb{P}(\operatorname{Cat}), \qquad \mathbb{P}(\operatorname{Dog})\] 

\end{itemize}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{ML пример}
%----------------------------------------------------------

\Large Остаётся применить \textbf{теорему Байеса}, \[  \mathbb{P}\left( \operatorname{Cat} \mid \operatorname{Data} \right) =  \frac{ {\color{blue} \mathbb{P}\left(\operatorname{Data} \mid \operatorname{Cat} \right) \mathbb{P}\left( \operatorname{Cat}\right)} }{{\color{blue}  \mathbb{P}\left(\operatorname{Data} \mid \operatorname{Cat} \right) \mathbb{P}\left( \operatorname{Cat}\right) } + \mathbb{P}\left(\operatorname{Data} \mid \operatorname{Dog} \right) \mathbb{P}\left( \operatorname{Dog}\right)} \] 

\vspace{2mm}

Voil\`a, мы нашли вероятность котика $\smiley{}$.

\pause

\vspace{5mm}

\textit{Замечание.} Первое слагаемое в знаменателе --- всегда числитель.




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



\section{Правила суммы и произведения}

\begin{frame}[plain]\frametitle{Правила суммы и произведения} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Учимся складывать и умножать}
%----------------------------------------------------------

Вспомним 2 важных факта --- как {\color{blue} складывать} и  {\color{blue}умножать} в  Теории Вероятностей. 

\vspace{5mm}


\textbf{Замечание}. В лекциях по Байесовским методам будем обозначать плотность вероятности через {\color{red} $p(x)$}, а не $f(x)$.

\vspace{5mm}


\begin{figure}[h!]
\center{\includegraphics[width=0.45\textwidth]{pic/BackToSchool}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Правило суммы}
%----------------------------------------------------------


{\color{red} Правило суммы}. Если $A_1, \dots, A_k$ --- разбиение $\Omega$, то \[ {\color{blue} \mathbb{P}(B) = \sum_{i=1}^k \mathbb{P} (B \mid A_i) \mathbb{P}(A_i)}.\]

В интегральном виде это записывается в виде \[p (b) = \int p(b, a) da  = \int p(b\mid a) p(a) da.\]

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------





%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Правило произведения}
%----------------------------------------------------------


{\color{red} Правило произведения}. Переписываем формулу условной вероятности: \[ {\color{blue} p(a, b) = p(a \mid b)p(b)}. \] 


\vspace{3mm}

В общем случае \[  p (x_{1},\ldots ,x_{n})=p(x_1 \mid x_{2}, \dots, x_n) p(x_2 \mid x_{3}, \dots, x_n) \dots p(x_{n-1} \mid x_n) p(x_n). \]



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Условные вероятности}
%----------------------------------------------------------

Зависимости между переменными удобно выражать диаграммой следующего вида:

\begin{figure}[h!]
\center{\includegraphics[width=0.35\textwidth]{pic/Gr2}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

Все условные вероятности вычисляются при помощи {\color{red} правил суммы и произведения}.

\vspace{5mm}

\textit{На семинаре} вычислим $p(y_5, y_7)$ при известных $y_1, y_2, y_4$ и неизвестных $y_3, y_6$. 



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Маргинализация и обуславливание}
%----------------------------------------------------------

\textit{Общее правило}. Пусть мы хотим вычислить $p(x)$ по $p(x, y)$.

\vspace{5mm}

\begin{itemize}

\item Если $y$ \textbf{неизвестно}, то мы {\color{blue} маргинализуем} по ним: \[ p(x) = \int p(x, y) dy\]

\vspace{5mm}


\item Если $y$ \textbf{известно}, то мы {\color{blue} обуславливаем} плотность по нему: \[ p(x \mid y) = \frac{p(x, y) }{p(y)}.\]



\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


\section{Байесовская вероятность}

\begin{frame}[plain]\frametitle{Байесовская вероятность} \tableofcontents[currentsection]\end{frame}



\subsection{Субъективная неопределённость}

\begin{frame}[plain]\frametitle{Субъективная неопределённость} \tableofcontents[currentsubsection]\end{frame}




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Частотная и байесовская статистики}
%----------------------------------------------------------

\begin{center} 
\Large Опишем Байесовский подход к Статистике. \\ Объясним --- чем он отличается от Частотного подхода.
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/BF1}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Частотная и байесовская статистики}
%----------------------------------------------------------

\begin{center} 
\Large 2 подхода к Статистике:
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{pic/2Models}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Частотная и байесовская статистики}
%----------------------------------------------------------

На самом деле люди рассуждают скорее ``по-байесовски''.

\vspace{5mm}

Мы не можем проводить бесконечную серию экспериментов.


\vspace{5mm}


\textbf{Пример}. Подбросили монетку. \textbf{Q:} С какой вероятностью выпадет орёл?

\begin{figure}[h!]
\center{\includegraphics[width=0.25\textwidth]{pic/Toss}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Частотная и байесовская статистики}
%----------------------------------------------------------



\begin{itemize}

\item \textbf{Частотный ответ:} Если подбрасывать бесконечное число раз, то в среднем будет $50\%$ орлов.

\vspace{5mm}


\item \textbf{Байесовский ответ:} Чем больше данных (направление и время полёта, скорость и т.д.) --- тем яснее, что выпадет.

\end{itemize}



\begin{figure}[h!]
\center{\includegraphics[width=0.25\textwidth]{pic/Toss}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




\subsection{Байесовский вывод}

\begin{frame}[plain]\frametitle{Байесовский вывод} \tableofcontents[currentsubsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Байесовский вывод}
%----------------------------------------------------------

\begin{center}
\Large {\color{red} Байесовский вывод}\end{center}


\vspace{3mm}

\begin{enumerate}

\item Фиксируем {\color{blue} априорное распределение} на параметры $p(\theta)$. 

\vspace{3mm}

Закладываем в него наши знания и представления о данных.


\vspace{3mm}

\pause

\item Выбираем статистическую модель $p(x \mid \theta)$ распределения данных.


\vspace{3mm}

\pause


\item Наблюдаем данные $x^n = (x_1, \dots, x_n)$  и вычисляем {\color{blue} апостериорное распределение} по \textbf{формуле Байеса} 
\[ {\displaystyle p(\theta \mid x^n)={\frac { p( x^n \mid \theta)\, p(\theta)}{\int  p( x^n \mid \theta)\,p(\theta) d\theta}}}.\]

 

\end{enumerate}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Байесовский вывод}
%----------------------------------------------------------


Для одного наблюдения формула Байеса имеет вид \[   {\displaystyle p(\theta \mid x)={\frac{p(x \mid \theta)\, p(\theta)}{\int  p(x \mid \theta)\,p(\theta) d\theta}}}. \]

\vspace{2mm}

\pause

Для независимых наблюдений {\color{blue} функция правдоподобия} распадётся в произведение:
\[ p( x^n \mid \theta) = \prod_{i=1}^n  p(x_i\mid \theta).\] 

\vspace{2mm}

Итоговая формула:\[ {\displaystyle p(\theta \mid x^n)={\frac {\prod_{i=1}^n p(x_i\mid \theta)\, p(\theta)}{\int \prod_{i=1}^n p(x_i\mid \theta)\,p(\theta) d\theta}}}.\]
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Нормировочная константа}
%----------------------------------------------------------


Посмотрим на формулу Байеса:\[ {\displaystyle p(\theta \mid x^n)={\frac { p( x^n \mid \theta)\, p(\theta)}{\int  p( x^n \mid \theta)\,p(\theta) d\theta}}}.\]


\vspace{2mm}
 
 \textbf{Q:} Зависит ли знаменатель от $\theta$?

\pause

\vspace{5mm}

\textbf{A:} Нет, это нормировочная константа, чтобы получалась вероятность:  \[ {\displaystyle p(\theta \mid x^n)={\frac {1}{Z} \, p( x^n \mid \theta)\, p(\theta)}}.\]


  %----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Пропорциональность}
%----------------------------------------------------------

\begin{center}
Равенство с точностью до умножения на константу обозначают знаком ``$\Huge{\color{red} \propto}$''.
\end{center}


\vspace{5mm}


\begin{center}
\noindent\fbox{%
    \parbox{27em}{%


\vspace{3mm}



    \Large \hspace{3mm} Апостериорное $\propto$ Правдоподобие $\cdot$ Априорное
    
\vspace{3mm}    
    }%
}


\end{center}

  %----------------------------------------------------------
\end{frame}
%----------------------------------------------------------







%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Перерыв}
%----------------------------------------------------------


\begin{center}
\Huge Перерыв
\end{center}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



\subsection{Выбор априорного распределения}

\begin{frame}[plain]\frametitle{Выбор априорного распределения} \tableofcontents[currentsubsection]\end{frame}

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Оставь надежду всяк сюда входящий}
%----------------------------------------------------------

\textbf{Q:} Как выбрать априорное распределение $p(\theta)$?

\vspace{5mm}

\textbf{A:} В него нужно ``вложить наши {\color{red} \textit{субъективные представления и знания}}'' о данных. 

\vspace{5mm}

Многих учёных ожидаемо ``триггерит'' от подобного.

  
\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/Subj2}}
\caption{\Large Добро пожаловать в ад Субъективизма} \label{Fig:}
\end{figure}
  
  %----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Несобственный прайор}
%----------------------------------------------------------

Один из ``универсальных'' выходов (не лучший).

\vspace{5mm}

Взять {\color{blue} несобственный прайор} --- пропорциональный константе \[ f(\theta) \propto c.\]



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Несобственный прайор}
%----------------------------------------------------------

Такое априорное может быть НЕ распределением (!) \[ \int f(\theta) d\theta = \infty.\]

\vspace{2mm}


А апостериорное --- уже возможно распределение \[ \int f(\theta \mid x^n ) \propto \mathcal{L}_n(\theta). \]

\begin{figure}[h!]
\center{\includegraphics[width=0.45\textwidth]{pic/Cho}}
%\caption{} \label{Fig:}
\end{figure}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Нормировочная константа}
%----------------------------------------------------------

\textbf{Q:} Что технически самое сложное в теореме Байеса? \[ {\displaystyle p(\theta \mid x^n)={\frac { p( x^n \mid \theta)\, p(\theta)}{\int  p( x^n \mid \theta)\,p(\theta) d\theta}}}.\]


\vspace{2mm}
 
 \textbf{A:} Посчитать интеграл:


\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/Integrals}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

  %----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Пайплайн}
%----------------------------------------------------------

При этом ``в идеале'' мы хотели бы ``поточное'' обновление параметров по мере поступления новых данных: 


\vspace{2mm}

\[{\color{blue} \operatorname{Apriori}_1} \xrightarrow{\operatorname{Data}_1} {\color{red} \operatorname{Aposteriori}_1} = {\color{blue} \operatorname{Apriori}_2} \xrightarrow{\operatorname{Data}_2} {\color{red} \operatorname{Aposteriori}_2}  = \dots\]


\vspace{2mm}

Рассмотрим случай, когда всё хорошо считается аналитически.

  %----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




\section{Сопряжённые распределения}

\begin{frame}[plain]\frametitle{Сопряжённые распределения} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Сопряжённые распределения}
%----------------------------------------------------------

Пусть 

\vspace{5mm}

\begin{itemize}

\item Априорное распределение $p(\theta)$ --- из семейства $\mathcal{A}$,

\vspace{5mm}

\item Функция правдоподобия $p(x^n \mid \theta)$--- из семейства $\mathcal{B}$.


\end{itemize}

\vspace{5mm}

Семейства $\mathcal{A}$ и $\mathcal{B}$ --- {\color{blue} сопряжённые}, если апостериорное распределение $p(\theta \mid x^n)$ тоже из семества $\mathcal{A}$.



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Сопряжённые распределения}
%----------------------------------------------------------

\textbf{Пример}. Рассмотрим бросание $n$ монеток, ему соответствует $\operatorname{Binomial}(n, \theta)$.

\vspace{2mm}

\begin{itemize}

\item Априорное распределение \[ \operatorname{Beta}(a, b) = C_1 {\color{blue} \theta^{a-1}} {\color{red} (1-\theta)^{b-1} }.\]

\vspace{2mm}

 Константы не важны --- \textit{для проверки сопряжения достаточно смотреть на вид выражения относительно $\theta$}.

\vspace{5mm}

\pause

\item Функция правдоподобия, если ``выпало $k$ орлов'': \[ \binom{n}{k} {\color{blue} \theta^k } {\color{red}  (1-\theta)^{n-k} }.\] 

\vspace{2mm}

\textbf{Q:} Какой вид у апостериорного?

\vspace{5mm}
\pause

\item Апостериорное распределение \[ \operatorname{Beta}(a+k, b+n-k) = C_2 {\color{blue} \theta^{a+k-1}} {\color{red} (1-\theta)^{b+n-k-1} }.\]


\end{itemize}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Сопряжённые распределения}
%----------------------------------------------------------


\begin{center} 

\Large Список сопряжённых распределений можно найти в Wikipedia:  

\vspace{5mm}

\url{https://en.wikipedia.org/wiki/Conjugate_prior} \end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.75\textwidth]{pic/Steal}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




\section{Трамвай}

\begin{frame}[plain]\frametitle{Трамвай} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Трамвай!}
%----------------------------------------------------------

\begin{center} Давайте поговорим про трамвай\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/Katz}}
\caption{Да, про трамвай} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Tramcar problem}
%----------------------------------------------------------

\begin{center} \LARGE  Приходите в город и видите трамвай под {\color{red} номером 100}.
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.95 \textwidth]{pic/Trolley}}
%\caption{Да, про трамвай} \label{Fig:}
\end{figure}

\begin{center} \LARGE  Естественно, у любого математика тут же возникает вопрос:

\pause

\vspace{5mm}

\Huge ``Сколько трамвайных маршрутов в городе?''

\end{center}





%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Tramcar problem}
%----------------------------------------------------------



\large Нужно выбрать (статистическую) модель для описания реальности. 

\vspace{5mm}


\textbf{Предположение}. В городе есть трамваи под номерами $1, \dots, N$. 

\vspace{5mm}


Трамваи независимы. Номера, которые мы видим, равновероятны. 


\vspace{5mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.8 \textwidth]{pic/Tram}}
%\caption{Да, про трамвай} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Частотный подход}
%----------------------------------------------------------


\begin{center}
\Large {\color{red} Частотный подход.}
\end{center}


\vspace{5mm}

\Large Наблюдаем за номерами трамваев $X_1, \dots, X_n$. 

\vspace{5mm}


Оцениваем $N$ как статистику от них.

\vspace{5mm}

Скажем, {\color{blue}оценка максимального правдоподобия}: \[ N_{ML} = X_{(n)} = \operatorname{max} (X_1, \dots, X_n). \] 

\vspace{5mm}

Логичный вывод: \textit{количество маршрутов --- самый большой номер $N$, что мы увидим за долгое время}.



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Байесовский подход}
%----------------------------------------------------------


\Large \textbf{Q:} С чем сопрягается дискретное равномерное распределение?

\vspace{2mm}

Смотрим Wikipedia --- беда!


\begin{figure}[h!]
\center{\includegraphics[width=0.8\textwidth]{pic/Bad}}
%\caption{Да, про трамвай} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Байесовский подход}
%----------------------------------------------------------

\large Ладно, меняем --- упрощаем модель.

\vspace{5mm}

Трамваи --- {\color{blue} непрерывно равномерно распределены от 0 до N}:  $ \operatorname{U}[0, N].$


\vspace{5mm}

\begin{figure}[h!]
\center{\includegraphics[width=0.5 \textwidth]{pic/Oki}}
%\caption{Да, про трамвай} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Распределение Парето}
%----------------------------------------------------------

Смотрим в Wiki --- что сопрягается с  $ \operatorname{U}[0, N]$?

\vspace{5mm}

{\color{blue} Распределение Парето:}  \[\operatorname{Pareto}(\theta|a,b) = \begin{cases} \displaystyle  \frac{ba^b}{\theta^{b+1}},& \qquad \theta \geq a, \\ 0, &\qquad \text{иначе}. \end{cases}\]


\begin{figure}[h!]
\center{\includegraphics[width=0.4 \textwidth]{pic/Pareto1}}
%\caption{Да, про трамвай} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Распределение Парето}
%----------------------------------------------------------


Распределение Парето часто используется в социологии и экономике.

\vspace{2mm}


 Изначально создавалось, чтобы описать распределение богатства по принципу 
 
 \vspace{2mm}
 
 
 \begin{center} \Large ``$20\%$ самых богатых владеет $80\%$ богатств''\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.5 \textwidth]{pic/Pareto}}
\caption{\Large Закон Парето --- принцип 80/20} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Функция правдоподобия}
%----------------------------------------------------------

Трамваи равномерны:
 \[ p(x)  = \displaystyle \frac{1}{\theta}, \qquad x \in [0, \theta] \]

\vspace{2mm}


Находим {\color{blue} функцию правдоподобия}, если наблюдали $M$ трамваев $x_1, \dots, x_M$: \[ L_{\theta}(x) = \prod_{i=1}^M p(x_i) = \begin{cases} \displaystyle  \frac{1}{\theta^M}, \qquad 0 \leq x_i \leq \theta, \quad i = 1, \dots, M, \\ 0, \qquad \text{иначе}. \end{cases}\]
        

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Апостериорное распределение}
%----------------------------------------------------------


\begin{itemize}

\item Априорное: \[ p(\theta) =  \frac{ba^b}{{\color{red} \theta^{b+1}}} I_{\theta \geq a}\]

\vspace{2mm}

\item Правдоподобие: \[ p(x_1,\dots,x_M| \theta) =  \frac{1}{{\color{red} \theta^M}} I_{\left\{0 \leq x_{(1)}  \leq x_{(M)} \leq \theta\right\}}.\]

\vspace{2mm}


\pause

\item По {\color{blue} Теореме Байеса} \[ p(\theta|x_1,\dots,x_M) \propto  p(x_1,\dots,x_M| \theta) p(\theta)\]  апостериорное   \[ p(\theta|x_1,\dots,x_M) \propto   \frac{ba^b}{{\color{red} \theta^{M + b+1}}} I_{\left\{ \theta \geq \max(a, x_{(M)})\right\}}.  \]

\end{itemize}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Физический смысл параметров}
%----------------------------------------------------------


    \textit{Параметры апостериорного распределения}: \[{\color{red}  a'=  \max(a, x_{(M)})}, \qquad {\color{blue} b' = b + M}.\]
    
    \vspace{2mm}
    
    Присмотревшись --- видим смысл параметров!
    
    \pause
    
    
    \vspace{2mm}
    
    
    \begin{itemize}
    
    \item {\color{red} $a$ --- наибольший номер, что мы видели};
    
    
    \vspace{2mm}
    
    
    \item {\color{blue} $b$ --- общее число увиденных трамваев}.
    
    \end{itemize}
    
    


\begin{figure}[h!]
\center{\includegraphics[width=0.5 \textwidth]{pic/Nit}}
%\caption{Да, про трамвай} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Физический смысл параметров}
%----------------------------------------------------------
    
\textbf{Упражение}. Для распределения $\operatorname{Pareto}(\theta|a,b) $ 

\vspace{5mm}

\begin{itemize}

\item Мода равна $a$

\vspace{5mm}

\item Матожидание равно $\displaystyle \frac{b a}{b-1}$.

\end{itemize}


\vspace{5mm}

Как ни смотри: {\color{blue} количество маршрутов --- самый большой номер $N$, что мы увидим за долгое время}.



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




\section{Применимость байесовского подхода}

\begin{frame}[plain]\frametitle{Применимость байесовского подхода} \tableofcontents[currentsection]\end{frame}



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Холиварная тема}
%----------------------------------------------------------

\begin{center} 
\Large Обсудим плюсы и минусы байесовского подхода.
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/Duel2}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}

\begin{center} 
\Large WARNING! Это холиварная тема!
\end{center}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Преимущества и недостатки}
%----------------------------------------------------------

\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{pic/hands}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Преимущества и недостатки}
%----------------------------------------------------------


\begin{center}
\Large Ещё плюсы и минусы Байесовского подхода:
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{pic/PM}}
%\caption{Парадокс Монти Холла} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{В пределе подходы совпадают}
%----------------------------------------------------------



На больших выборках байесовский подход согласуется с классическим частотным.

\vspace{5mm}

Мода апостериорного $\theta_{MP}$ (MAP-оценка) стремится к оценке максимального правдоподобия:

 \[ \lim_{n\to \infty} \theta_{MP} = \theta_{ML}\]

\vspace{5mm}


Подробнее --- см. Теорему 11.5 в  


\vspace{5mm}

\begin{thebibliography}{10}

    
  \beamertemplatebookbibitems
  \bibitem{Wasserman} Wasserman L. 
  \newblock{\em All of Statistics}.

    

  \end{thebibliography}
  
 

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Резюме}
%----------------------------------------------------------


\begin{center}
\Large Байесовский подход --- зачастую не самый эффективный, но по-своему интригующий и завораживающий взгляд на ML.
\end{center}

\vspace{5mm}
 
\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{pic/BB1}}
%\caption{Не тот Симпсон} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Abandon All Hope}
%----------------------------------------------------------


\begin{center}
\Large Мы начали наше приключение в ``Байесовском мире''.

\vspace{2mm}

Что нас ждёт в конце?

\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.75\textwidth]{pic/Abandon2}}
%\caption{Не тот Симпсон} \label{Fig:}
\end{figure}





%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


\end{document}


