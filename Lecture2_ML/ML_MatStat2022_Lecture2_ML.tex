\documentclass[9pt]{beamer}




\usepackage{beamerthemesplit}
\usetheme{Boadilla}
\usecolortheme{orchid}





 % \usepackage[cp866]{inputenc}                %%% Є®¤Ёа®ўЄ  DOS
  \usepackage[cp1251]{inputenc}
%  \usepackage[T2A]{fontenc}                %%% ???
%\usepackage[english,russian]{babel}
%\usepackage[russian]{babel}
 \usepackage[OT1]{fontenc}
%\usepackage[english]{babel}
\usepackage[russian]{babel}

\usepackage{amssymb,latexsym, amsmath, mathdots}

\usepackage{amscd}
\usepackage{multirow}
\usepackage{comment}

\usepackage{epstopdf}



%\usepackage[table]{xcolor} %colored cells

%\usepackage{tikz}
%\usetikzlibrary{tikzmark} %arrows in tables


\usepackage{mnsymbol} % udots in matrices


 \usepackage{pgfpages}
%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=bottom}
%% \setbeameroption{show notes on second screen}

%\setbeamercolor{alerted text}{fg=red!80!black}


% \setbeamercolor{alerted text}{fg=blue!50!green}
% \setbeamercolor{fortheorem}{bg=blue!15!white}
% \setbeamercolor{EMPF}{bg=blue!25!white}
% \setbeamercolor{EMPF2}{bg=blue!35!white}

%\setbeamersize{text margin left=5.mm, text margin right=5.mm}


%---------------------------------------------------------------------------------------------------------------------------

%\def\emphbox#1#2#3{\begin{beamercolorbox}[wd=#2, ht=#1, center, colsep=1.mm]{EMPF} #3 \end{beamercolorbox}}

%---------------------------------------------------------------------------------------------------------------------------


%\advance\leftskip-5.mm



%***************************************************************************************************************************

\theoremstyle{theorem}
\newtheorem{mytheorem}[theorem]{Теорема}
\newtheorem{mylemma}[theorem]{Лемма}
\newtheorem{mycorollary}[theorem]{Следствие}

\newtheorem{proposition}[theorem]{Предложение}
\newtheorem{assertion}[theorem]{Утверждение}
\newtheorem{remark}[theorem]{Замечание}
\newtheorem{assumption}[theorem]{Предположение}

\newtheorem{convention}[theorem]{Договорённость}
\newtheorem{question}[theorem]{Вопрос}
%\newtheorem{mydefinition}{mydefinition}

\theoremstyle{definition}
\newtheorem{mydefinition}[theorem]{Определение}
\newtheorem{myexample}[theorem]{Пример}



%***************************************************************************************************************************

\chardef\No=194
\newcommand{\bd}{{\rm bd}}
\newcommand{\cl}{{\rm cl}}
\newcommand{\ind}{{\rm ind}}
\newcommand{\id}{{\rm id}}
\newcommand{\inj}{{\sl in}}
\newcommand{\pr}{{\rm pr}}
\newcommand{\st}{{\rm St}}


\DeclareMathOperator{\sgrad}{sgrad}

\DeclareMathOperator{\cnt}{const}

\DeclareMathOperator{\Aut}{Aut}


\DeclareMathOperator{\sgn}{sgn}

\DeclareMathOperator{\Int}{Int}


\DeclareMathOperator{\tr}{tr}

\DeclareMathOperator{\Mat}{Mat}







%****************************************************************************
\newcommand{\Ker}{\mathrm{Ker}\,\,}
\newcommand{\Ann}{\mathrm{Ann}\,\,}

\newcommand{\Imm}{\mathrm{Im}\,\,}

\newcommand{\rk}{\mathrm{rk}\,\,}

\newcommand{\const}{\mathrm{const}}



\DeclareMathOperator{\Pen}{\mathcal{P}}


\DeclareMathOperator{\Core}{K}


\DeclareMathOperator{\BLG}{\operatorname{BLG}}

\DeclareMathOperator{\AutBP}{\operatorname{Aut}(V, \mathcal{P})}


\def\minus{\hbox{-}}   %Sign of minus in big matrices

\usepackage{multirow}  %Columns spanning multiple rows

\usepackage[normalem]{ulem} %underline that can break lines

%\usepackage[normalem]{ulem}

%****************************************************************************



\title% [] (optional, only for long titles)
{Прикладная статистика в машинном обучении \\ Лекция 2 \\ Вероятностный взгляд на ML}
%\subtitle{}
\author [И. \,К.~Козлов] %(optional, for multiple authors)
{И. \,К.~Козлов \\ {\footnotesize(\textit{Мехмат МГУ})}}


\date % [](optional)
{2022}
%\subject{Mathematics}








\usepackage{hyperref}
\hypersetup{unicode=true}



\begin{document}

%\includeonlyframes{}
\frame{\titlepage}



\section{Что такое ML?}



\begin{frame}[plain]\frametitle{Что такое ML?} \tableofcontents[currentsection]\end{frame}




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Machine Learning}
%----------------------------------------------------------


\begin{figure}[h!]
\center{\includegraphics[width=0.8\textwidth]{Pic/MLDef}}
%\caption{\Large Предстоящая эволюция в понимании задачи} \label{Fig:}
\end{figure}


\begin{center} \Huge Что такое ML? Какие в нём задачи? \end{center}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Обучение с учителем}
%----------------------------------------------------------

Одна из типичных задач машинного обучения --- {\color{blue} обучение с учителем}.

\vspace{5mm}

\begin{itemize}

\item  Дана {\color{blue} обучающая выборка} --- набор пар $(x_1, y_1), \dots, (x_n, y_n)$

\vspace{5mm}

\item $x_i \in X$ --- это объекты, $y_i \in Y$ --- их метки (классы).
\vspace{5mm}

\item \textbf{Наша задача} --- построить {\color{blue} решающую функцию} \[a: X \to Y,\]  как можно лучше предсказывающую метки. 

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Регрессия и классификация}
%----------------------------------------------------------

Типичные задачи машинного обучения:

\vspace{5mm}


\begin{itemize}

\item Регрессия $Y = \mathbb{R}$.

\vspace{5mm}

\item Классификация $Y = \left\{ 1, \dots, M\right\}$


\end{itemize}


\vspace{5mm}

Конечно, есть и другие задачи. 


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Ранжирование}
%----------------------------------------------------------


\begin{center}
\Huge Ранжирование. 
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{Pic/Search}}
%\caption{\Large Предстоящая эволюция в понимании задачи} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Кластеризация}
%----------------------------------------------------------

\begin{center}
\Huge Кластеризация (обучение без учителя). 
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{Pic/Clust}}
%\caption{\Large Предстоящая эволюция в понимании задачи} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{RL}
%----------------------------------------------------------



\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{Pic/RL}}
%\caption{\Large Предстоящая эволюция в понимании задачи} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Признаки}
%----------------------------------------------------------


Каждый объект $x_i$  задаётся своими признаками $x_{i1}, \dots, x_{ip}$.  

\vspace{5mm}

Для простоты будем считать, что все признаки --- числа $f_i \in \mathbb{R}$ ({\color{blue} количественные} признаки).

\vspace{5mm}

На практике многие признаки --- {\color{blue} категориальные}, т.е. принимают конечное число значений (мужчина/женщина, группа крови, регион и т.д.)


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{По матрице предсказываем таргет}
%----------------------------------------------------------

Итак, нам дана {\color{blue} матрица объектов-признаков } 

\vspace{3mm} 

\[ \begin{pmatrix} x_{11}&\cdots &x_{1p}\\ x_{21}&\cdots &x_{2p}\\ \vdots &\ddots &\vdots \\ x_{n1}&\cdots &x_{np}\end{pmatrix} \] 

\vspace{3mm} 


и мы пытаемся по ней предсказать метки $y_i$.




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Линейная регрессия}
%----------------------------------------------------------

\begin{center} Рассмотрим простейшую задачу {\color{blue} линейной регрессии} \[ {\displaystyle y_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+\varepsilon _{i},\qquad i=1,\ldots ,n,}\]


%\[{\displaystyle y_{i}= \mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i},\qquad i=1,\ldots ,n,} \]

\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{pic/SimpREg}}
%\caption{$y_i=\mathbf {x} _{i}^{\mathsf {T}}{\boldsymbol {\beta }}+\varepsilon _{i}$} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Линейная регрессия}
%----------------------------------------------------------


Вместе все эти уравнения можно записать в матричном виде  \[{\displaystyle \mathbf {y} =X{\boldsymbol {\beta }}+{\boldsymbol {\varepsilon }},\,}\]


\[ {\displaystyle \mathbf {y} ={\begin{pmatrix}y_{1}\\y_{2}\\\vdots \\y_{n}\end{pmatrix}},\quad }
{\displaystyle {\boldsymbol {\beta }}={\begin{pmatrix}\beta _{0}\\\beta _{1}\\\beta _{2}\\\vdots \\\beta _{p}\end{pmatrix}},\quad {\boldsymbol {\varepsilon }}={\begin{pmatrix}\varepsilon _{1}\\\varepsilon _{2}\\\vdots \\\varepsilon _{n}\end{pmatrix}}.} \]

\vspace{3mm} \underline{Подчеркнём} --- чтобы учесть свободный член, в 1ом столбце матрицы $X$ стоят 1:
\[ {\displaystyle X={\begin{pmatrix}\mathbf {x} _{1}^{\mathsf {T}}\\\mathbf {x} _{2}^{\mathsf {T}}\\\vdots \\\mathbf {x} _{n}^{\mathsf {T}}\end{pmatrix}}={\begin{pmatrix}1&x_{11}&\cdots &x_{1p}\\1&x_{21}&\cdots &x_{2p}\\\vdots &\vdots &\ddots &\vdots \\1&x_{n1}&\cdots &x_{np}\end{pmatrix}},}\]

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Эволюция на сегодня}
%----------------------------------------------------------

\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{Pic/Evo}}
\caption{\Large Предстоящая эволюция в понимании задачи} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


\section{Линейная алгебра}

\begin{frame}[plain]\frametitle{Линейная алгебра} \tableofcontents[currentsection]\end{frame}

%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Линал}
%----------------------------------------------------------

\begin{center}
Начнём с доисторических методов (1ый курс). 
\end{center}




\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{Pic/Monkey}}
%\caption{Рис. 1} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{СЛУ}
%----------------------------------------------------------

\textbf{Q:} Как решать СЛУ \[ Ax = b?\]

\vspace{5mm}

\textbf{A:} Правильно, \[ x = A^{-1}b.\]

\vspace{5mm}

\textbf{Q2:} А если матрица $A$ не квадратная?


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Псевдорешение}
%----------------------------------------------------------




Для любого ЛСУ \[Ax = b\] существует {\color{blue} псевдорешение} \[A^{+}b.\] Это

\vspace{5mm}

\begin{enumerate}

\item решение методом наименьших квадратов \[\left\|A x -  b\right\| \to \min, \] 

\vspace{5mm}

\item наименьшей длины \[ \left\|x\right\| \to \min.  \] 

\end{enumerate}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Псевдообратная матрица}
%----------------------------------------------------------

Псевдообратную матрицу $A^{+}$ легко определить через \textbf{SVD-разложение}. С его помощью доказываются многие теоретические факты. 

\vspace{5mm}

Потом приведём более простые формулы для $A^{+}$ в частных случаях. 



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------






%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Сингулярное разложение}
%----------------------------------------------------------

\begin{center}
{\color{blue} \Huge SVD-разложение:}  
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.95\textwidth]{Pic/SVD2}}
%\caption{Рис. 1} \label{Fig:}
\end{figure}



\begin{center}
Напомним, матрица $U$ ортогональная, если $U^T U = E$.
\end{center}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Сингулярное разложение}
%----------------------------------------------------------

{\color{blue} SVD-разложение} над $\mathbb{C}$ аналогично:   \[ A = U \Sigma \bar{V}^T,\]

\begin{enumerate}

\item $U, V$ -- унитарные матрицы \[ U \bar{U}^T = E, \qquad V \bar{V}^T = E.\]

\vspace{3mm}

\item все значения диагональной матрицы $\Sigma$ вещественны и неотрицательны:  \[ \sigma_1 \geq \dots \geq \sigma_r > 0.\]

\end{enumerate}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{SVD разложение}
%----------------------------------------------------------


\begin{mytheorem} Для любой (вещественной или комплексной) $n\times m$-матрицы $A$ существует SVD разложение. \end{mytheorem}


\vspace{5mm}

У нас матрицы будут вещественными. 

\vspace{5mm}

Доказательство (или ссылки на него) см. Wiki:\url{https://en.wikipedia.org/wiki/Singular_value_decomposition}.


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------





%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Псевдообратная через SVD}
%----------------------------------------------------------

Псевдообратная через SVD \[ A = U \Sigma V^T.\] 

Тогда {\color{blue} псевдообратная матрица:} \[ A^{+} = V  \Sigma^{+} U^T,\] где \[  \Sigma^{+} =  \left(\frac{1}{\sigma_1}, \dots, \frac{1}{\sigma_k}, 0, \dots, 0\right).\]


\pause

\vspace{3mm}


\textbf{Q:} $A$ --- это $m \times n$ матрица, какой размер $A^{+}$?


\vspace{5mm}

\pause

$A^{+}$ --- это $n \times m$ матрица. 

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Псевдообратная матрица}
%----------------------------------------------------------


Алгоритмы нахождения псевдообратной матрицы (в частных случаях):

\vspace{5mm}


\begin{itemize}


\item Если $A$ --- квадратная и обратимая \[ A^{+} = A^{-1}\]

\end{itemize}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Псевдообратная матрица}
%----------------------------------------------------------


\begin{itemize}

\item Пусть столбцы $A$ линейно независимы. Тогда $A^T A$ обратима. 

\vspace{5mm}

Домножаем уравнение $Ax = b$ на $A^T$, получаем \[ A^T A x = A^T b.\]

\vspace{3mm}

\textit{Формула для псевдообратной матрицы} {\color{red} \[A^+ = (A^T A)^{-1} A^T.\]}

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Псевдообратная матрица}
%----------------------------------------------------------




\begin{itemize}

\item Аналогично, если строчки $A$ линейно независимы, то $AA^T$ обратима и {\color{red}  \[A^+ = A^T(A A^T)^{-1}.\]}

\vspace{3mm}

Кратко: если любая из 2 формул выше вычислима, то она верна. 

\end{itemize}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



\section{ML решение}

\begin{frame}[plain]\frametitle{ML решение} \tableofcontents[currentsection]\end{frame}


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Инженерный подход}
%----------------------------------------------------------

\begin{center}
Теперь обсудим инженерный подход. 
\end{center}



\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{Pic/Engineer}}
%\caption{Рис. 1} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Погрешность вычислений}
%----------------------------------------------------------


Формулу $A^+ = (A^T A)^{-1} A^T$ трудно вычислять на практике.


\vspace{5mm}

Много делений/умножений $\qquad \Rightarrow \qquad$ большая погрешность вычислений. 



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Стандартный пайплайн}
%----------------------------------------------------------

\begin{center}
\Large Стандартный пайплайн в вычислительных задачах: 
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{Pic/Pipe1}}
%\caption{Рис. 1} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Стандартный пайплайн}
%----------------------------------------------------------

Насколько хорошо наше предсказание?

\vspace{5mm}

Вводим квадратичную {\color{blue} функцию потерь} \footnote{ MSE = mean squared error} \[ \mathcal{L} = \frac{1}{n} \sum_{(x_i,y_i)} \left(y_i - prediction(x_i)\right)^2.\]

\vspace{5mm}

Для линейной регрессии решение --- минимум этой функции: \[ {\hat {\beta }} ={\underset {\beta }{\mbox{arg min}}} \, \mathcal{L} ={\underset {{\beta }}{\mbox{arg min}}}\sum _{i=1}^{n}\left( y_i -\mathbf {x} _{i}^{\mathsf {T}}{{\beta }} \right)^{2} \]

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Metric vs Loss function}
%----------------------------------------------------------

\begin{center}
\Huge Не перепутайте:
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{Pic/Money}}
%\caption{Рис. 1} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Градиент}
%----------------------------------------------------------


\textbf{Утверждение}. {\color{red} Градиент --- направление наибольшего роста функции. }

\vspace{5mm}

Дана функция $f(x^1, \dots, x^n): \mathbb{R}^n \to \mathbb{R}$. Её градиент: \[ \left( \frac{\partial f}{\partial x^1}, \dots,  \frac{\partial f}{\partial x^n} \right).\]

\vspace{5mm}

Производная по направлению $v$ (в нуле): \[ \frac{d}{dt} f(tv^1, \dots, tv^n) = \sum_{i=1}^n \frac{\partial f}{\partial x^i} v^i = (\operatorname{grad} f, v). \] 


\pause 
\vspace{3mm}

Напомним: \[ (u, v) = |u| |v| \cos{\varphi}\] Поэтому скалярное произведение максимально, если векторы сонаправлены.
 
 
 
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Градиентные методы}
%----------------------------------------------------------

Поэтому для поиска оптимума можно использовать различные {\color{blue} градиентные методы} 
 \[ \beta_j \to \beta_j - \alpha \frac{\partial}{\partial \beta_j} \mathcal{L}.\] На практике формулы посложнее.
 
 \vspace{5mm}
 
 \pause 
 
 \textbf{Упр.} Проверить, что градиент для функции \[ \mathcal{L}(\beta, X, y) = \left\| y - X \beta \right\|^2 \] имеет вид \[\nabla_{\beta} \mathcal{L} =  X^T (X \beta - y).  \] 
 
 
 
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




\section{Задача классификации}

\begin{frame}[plain]\frametitle{Задача классификации} \tableofcontents[currentsection]\end{frame}



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Классификация}
%----------------------------------------------------------

Поговорим про \textbf{задачу классификации}. Пусть классов всего два: 0 и 1. 

\vspace{5mm}

Для классификации достаточно предсказывать вероятность класса $\mathbb{P}(y_i = 1 \big| x)$. 




\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{Pic/CatDog}}
\caption{\Large Предсказываем вероятности классов} \label{Fig:}
\end{figure}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Сигмоида}
%----------------------------------------------------------

\begin{center}
Переход: регрессия $\to$ классификация. 
\end{center}

\vspace{3mm}

Достаточно отобразить предсказания из $\mathbb{R}$ в вероятности из $[0,1]$.  

\vspace{3mm}

Удобной оказывается {\color{blue} сигмоида} \[\sigma(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}} \]


\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{Pic/Sigmoid2}}
%\caption{\Large Предсказываем вероятности классов} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Логистическая регрессия}
%----------------------------------------------------------

{\color{blue} Логистическая регрессия:}

\[ {\displaystyle p={\frac {1}{1+b^{-(\beta _{0}+ \sum \beta _{i}x_{i} )}}}}\]

\vspace{3mm}

\pause

В качестве функции потерь берётся {\color{blue} логлосс} (logloss): 

\[ 	L_{\operatorname{log}}(y, p) = - \left( y\log p  + (1-y)\log(1-p) \right).\]

\vspace{3mm}

Другое название: {\color{blue} перекрёстная кросс-энтропия} (Cross Entropy).


\vspace{5mm}

Причину использования этого лосса {\color{cyan} обсудим на семинаре}.

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Катарсис}
%----------------------------------------------------------



\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{Pic/Catarsis}}
\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Deep Learning}
%----------------------------------------------------------

\begin{center}
\textbf{Q:} \Large Знаете ли Вы, что такое нейронная сеть/Deep Learning?
\end{center}

\vspace{5mm}

\pause

\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{Pic/DL}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Регрессии --- нейронки}
%----------------------------------------------------------


\begin{center} 
\Large \textit{Линейная регрессия --- это однослойная нейронная сеть.}
\end{center}

\begin{center} 
``Док-во''. Полносвязный слой --- это линейное отображение (см. сайт pytorch). 
\end{center}

\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{Pic/FCLin}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Регрессии --- нейронки}
%----------------------------------------------------------


\textit{\Large Логистическая регрессия --- тоже нейронная сеть:} 

\begin{center} \Large  полносвязный слой + сигмоида \end{center}

\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{Pic/LogNeur}}
%\caption{\Large Линейная и логистическая регрессии --- это нейронки} \label{Fig:}
\end{figure}



Сигмоида играет роль {\color{blue} функции активации} в нейроне.



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Перерыв}
%----------------------------------------------------------


\begin{center}
\Huge Перерыв
\end{center}
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



\section{Пара инженерных проблем}

\begin{frame}[plain]\frametitle{Пара инженерных проблем} \tableofcontents[currentsection]\end{frame}




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Много вычислений}
%----------------------------------------------------------

С линейной регрессией возникает пара проблем:


\vspace{5mm}

\begin{enumerate}
\item \textbf{Долго вычислять градиент}.


\vspace{5mm} 

 На каждом шаге нужно искать градиент функции потерь $\nabla \mathcal{L}$.

\vspace{5mm} 
 
 А это сумма большого числа слагаемых \[\mathcal{L} = \frac{1}{n} \sum_{i=1}^n \mathcal{L} (\beta, x_i, y_i).\]
\end{enumerate}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Стохастический градиентный спуск}
%----------------------------------------------------------



\begin{center} 
Общий подход в программировании: 

\vspace{5mm}

{\color{red} \Large Хотим ускорить процесс --- распараллеливаем его.}
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{Pic/Parallel}}
%\caption{\Large Линейная и логистическая регрессии --- это нейронки} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Стохастический градиентный спуск}
%----------------------------------------------------------


{\color{blue} Стохастический градиентный спуск}: заменяет градиент  \[ \nabla \mathcal{L} = \frac{1}{n} \sum_{i=1}^n \nabla \mathcal{L} (\beta, x_i, y_i)\] на одно слагаемое \[ \nabla_j = \nabla \mathcal{L} (\beta, x_j, y_j).\]

\vspace{3mm}

\pause

На практике берут не одно слагаемое, а среднее по {\color{blue} мини-батчу} \[\nabla \mathcal{L} \approx \frac{1}{B} \sum_{t=1}^B \nabla \mathcal{L} (\beta, x_{i_t}, y_{i_t}).\]


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Стохастический градиентный спуск}
%----------------------------------------------------------

\begin{figure}[h!]
\center{\includegraphics[width=\textwidth]{Pic/Stoch}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Алгоритм Роббинса-Монро}
%----------------------------------------------------------




Математически сходимость обосновывается {\color{blue} алгоритмом Роббинса-Монро}.


\vspace{5mm}

Важно, что градиент $\nabla \mathcal{L}$ заменяется на его \textit{несмещённую оценку} в этой точке 
\[\nabla_B = \frac{1}{B} \sum_{t=1}^B \nabla \mathcal{L} (\beta, x_{i_t}, y_{i_t}), \qquad \Rightarrow \qquad  \mathbb{E} \nabla_B  = \nabla \mathcal{L} \]


\vspace{5mm}

Очень грубо говоря, \textit{в среднем мы идём в правильном направлении}.



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Мультиколлинеарность}
%----------------------------------------------------------

2ая проблема с линейной регрессией:


\vspace{5mm}

\begin{enumerate}

\setcounter{enumi}{1}

\item \textbf{Мультиколлинеарность}.


\end{enumerate}

\vspace{5mm}

Пусть признаки зависимы, скажем, \[ x_1 = x_2 + x_3. \]

\vspace{3mm}

Тогда к решению можно прибавлять тривиальное выражение вида \[ \alpha x_1 - \alpha(x_2 + x_3).\]


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Мультиколлинеарность}
%----------------------------------------------------------

Выражение \[(X^T X)^{-1} X^T\] нельзя посчитать, если матрица $(X^T X)$ вырождена (или близка к вырожденной).

\vspace{5mm}

\pause 

\textbf{Q:} Как выглядит следующее множество? \[ \left\| X \beta - y \right\| \to \min \] 


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Мультиколлинеарность}
%----------------------------------------------------------

Линал учит нас, что решение $X \beta = y$ --- это подпространство.

\vspace{5mm}

Общее решение = частное решение + $\operatorname{Ker} X$: \[X \hat{\beta} = y, \quad X \gamma = 0 \qquad \Rightarrow \qquad X(\hat{\beta} + \gamma) = y.\]

\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{Pic/Affine}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------





%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Инженерное решение}
%----------------------------------------------------------


\begin{center}
\Large Как решить проблему?
\end{center}


\begin{figure}[h!]
\center{\includegraphics[width=0.5\textwidth]{Pic/DuckTape}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Регуляризация}
%----------------------------------------------------------


{\color{blue} Регуляризуем} лосс --- добавляем слагаемое, чтобы ограничить модуль решения.


\vspace{5mm}


\begin{enumerate}

\item $L_2$-регуляризация ({\color{blue} Ridge regression}) \[\hat{\mathcal{L}} = \mathcal{L} + \lambda \sum^n_{\color{red} j=1} \beta_j^2.  \] 

\pause

\vspace{3mm}

\item  $L_1$-регуляризация ({\color{blue} Lasso regression}) \[\hat{\mathcal{L}} = \mathcal{L} + \lambda  \sum^n_{\color{red} j=1} \left|\beta_j\right|.  \] 

\end{enumerate}

\vspace{3mm}

$\lambda $ --- подбираемый гиперпараметр.

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{LASSO отбирает признаки}
%----------------------------------------------------------

\textbf{Q:} Откуда название LASSO?

\vspace{5mm}

\textbf{A:} При изменении $\lambda$ происходит {\color{red} отбор признаков} --- $\beta_i$ постепенно обнуляются.

\begin{figure}[h!]
\center{\includegraphics[width=0.85\textwidth]{Pic/L1}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------





\section{Вероятностный подход}



\begin{frame}[plain]\frametitle{Вероятностный подход} \tableofcontents[currentsection]\end{frame}




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Вероятностные модели}
%----------------------------------------------------------

Общая вероятностная постановка задачи ML:

\vspace{5mm}

\begin{itemize}


\item Есть неизвестная вероятностная плотность $p(x, y)$. 

\vspace{5mm}

\item Пытаемся аппроксимировать её плотностью $\varphi(x, y, \theta)$. 

\vspace{5mm}

\item Делаем это, по \textit{принципу максимума правдоподобия}: \[ \mathcal{L} = \prod_{i=1}^n \varphi(x_i, y_i, \theta) \to \max.\] 

\end{itemize}




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Вероятностные модели}
%----------------------------------------------------------

По сути стандартная задача статистики. 

\vspace{5mm}


\textbf{Q:} Чем полезен вероятностный подход? Его плюсы и минусы?




%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Преимущества и недостатки}
%----------------------------------------------------------

\textbf{Недостатки:}

\vspace{5mm}

\begin{enumerate}

\item Сложнее. 


\vspace{5mm}


\item ``Игра часто не стоит свеч'' (костыли и эвристики for the win). 

\end{enumerate}

\vspace{5mm}

\pause

\textbf{Преимущества:}


\vspace{5mm}

\begin{enumerate}

\item Можно добавлять априорные знания.

\vspace{5mm}

\item Модели допускают пропуски в данных.

\vspace{5mm}

\item Если знаем вероятность $p(y|x)$ метки $y$ для данного объекта $x$, то можем только предсказать метку $y$. 

\vspace{5mm}

А зная совместную плотность $p(x,y)$, можно генерировать (!) объекты.

\end{enumerate}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{2 типа моделей}
%----------------------------------------------------------




\begin{figure}[h!]
\center{\includegraphics[width=0.75\textwidth]{Pic/GenDis}}
\caption{\Large Генеративные VS Дискриминативные модели} \label{Fig:}
\end{figure}


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Зашумлённое предсказание}
%----------------------------------------------------------


Считаем, что:

\vspace{5mm}

\begin{itemize}

\item Значения (таргет), которые мы наблюдаем, \textit{зашумлены}: \[y_i = f(x_i, \omega) + \varepsilon_i\] 

\vspace{3mm}


\item $\varepsilon_i$ --- это гауссовский шум, т.е. он имеет нормальное распределение \[\varepsilon_i \bigr| X \sim \mathcal{N}(0, \sigma^2).\]

\vspace{3mm}

\item Шумы независимы и одинаково распределены.

\end{itemize}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Зашумлённое предсказание}
%----------------------------------------------------------


\begin{figure}[h!]
\center{\includegraphics[width=0.75\textwidth]{Pic/Noise}}
%\caption{} \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------




%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Условная вероятность}
%----------------------------------------------------------

Мы будем использовать {\color{blue} условные вероятности}.

\vspace{5mm}

Подробно обсудим их потом. Выражение вида \[ p(y_i \bigr| x_i, \omega)\] следует понимать как ``вероятность $y_i$ при фиксированных $x_i, \beta$''. 


\vspace{5mm}

Можно считать это формальным обозначением. \textit{Зафиксируем $x_i, \omega$}. 


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


 
%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Правдоподобие}
%----------------------------------------------------------

Вероятность $y_i =f( x_i, \omega) + \varepsilon_i$, такая же, как и у шума $\varepsilon_i$: 
\[  p(y_i |x_i, \omega) = \frac{1}{\sqrt{2 \pi \sigma^2}} \operatorname{exp} \left( - \frac{\left(y_i - f( x_i, \omega)  \right)^2}{2 \sigma^2}\right).\]

 \vspace{3mm}

Посмотрим на {\color{red} оценку максимума правдоподобия}. 

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{MAE}
%----------------------------------------------------------

\begin{itemize}

\item  (Условное) правдоподобие: 

\[  L = \prod_{i}  p(y_i |x_i, \omega) = \prod_{i} \frac{1}{\sqrt{2 \pi \sigma^2}} \operatorname{exp} \left( - \frac{(y_i-f( x_i, \omega)^2)}{2\sigma^2}\right)\]
 
 \vspace{3mm}
 
 \pause 
 
 \item  Логарифм правдоподобия: 
\[  \ell = - \sum_{i}  \frac{(y_i - f( x_i, \omega)^2)}{2\sigma^2} + \operatorname{const}. \]

\end{itemize}

\vspace{3mm}

\textbf{Q:} Мы видели ранее похожее выражение?


%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Max Likelihood = Min MSE}
%----------------------------------------------------------

Максимизация правдоподобия \[ \ell = - \sum_{i}  \frac{(y_i- f( x_i, \omega))^2)}{2\sigma^2} + \operatorname{const} \] эквивалентна минимизации MSE-ошибки \[ \operatorname{MSE} = \frac{1}{n} \sum_i  (y_i -f( x_i, \omega)^2).\]  




 
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------


%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Подведём итоги}
%----------------------------------------------------------
\begin{center}
\noindent\fbox{%
    \parbox{21em}{%


\vspace{3mm}

\Large \hspace{4mm} В предположении нормальности:

\vspace{3mm}



    \Huge \textbf{\hspace{1mm} ОМП = оценка МНК}
    
\vspace{3mm}    
    }%
}


\end{center}

%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------



%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Регуляризация и вероятность}
%----------------------------------------------------------

\begin{center}
\Huge А где в вероятностной схеме регуляризация?
\end{center}

 
\begin{figure}[h!]
\center{\includegraphics[width=0.65\textwidth]{Pic/Conf}}
%\caption{\Large Сейчас будет катарсис} \label{Fig:}
\end{figure}
 
 
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------







%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Что дальше?}
%----------------------------------------------------------
 
 
 \begin{center}
{ \color{red} Обсудим во 2ой части курса}.
 \end{center}
 
 
 \vspace{5mm}
 
\begin{center} 
Если ввести правильное априорное распределение на веса $p(\beta)$ \\ и применить {\color{blue} теорему Байеса}
\end{center} 

 \vspace{3mm}

\[ p(\beta \, \bigr| \, \operatorname{Data}) =   \frac{p(\operatorname{Data} \, \bigr| \, \beta  ) p(\beta) }{p(\operatorname{Data})} \]

 \vspace{3mm}

\begin{center} 
получатся формулы для регуляризации. 
\end{center} 


 
%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------







%----------------------------------------------------------
\begin{frame}[plain]\frametitle{Что дальше?}
%----------------------------------------------------------


\begin{center} 
\large Объяснит ли теорема Байеса регуляризацию? 

Какие новые модели мы узнаем?
\end{center} 


\begin{figure}[h!]
\center{\includegraphics[width=0.4\textwidth]{Pic/Bayes}}
%\caption{\Large } \label{Fig:}
\end{figure}



%----------------------------------------------------------
\end{frame}
%----------------------------------------------------------





\end{document}

